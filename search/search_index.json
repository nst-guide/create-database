{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NST Guide Data \u00b6 Overview \u00b6 This repository contains code for data pipelines to generate map waypoints and layers of interest from open map data sources. Data Sources \u00b6 Town Boundaries: for now, these are drawn by hand using local trail knowledge and geojson.io and saved to data/pct/polygon/bound/town/{ca,or,wa}/*.geojson . OpenStreetMap : I use OSM for trail information and town waypoints. Initially, I planned to download whole-state extracts from Geofabrik . After discovering the osmnx package for Python, I decided to use that instead. That calls OSM's Overpass API , and then helpfully manages the result in a graph. This has a few benefits: No need to download any large files. Using the Geofabrik extracts, California is nearly 1GB of compressed data, and most of that is far from the trail, and really not necessary for this project. Speed. Unsurprisingly, when you're working with 1GB of compressed data just for california, computations aren't going to be super fast. Faster updates. Geofabrik extracts are updated around once a week I think, while the Overpass API has near-instant updating. That means that if I fix an issue with the data in OSM's editor, then I can get working with the new data immediately. Halfmile : Halfmile has accurate route information and a few thousand waypoints for the trail. I have yet to hear final confirmation that this is openly licensed, but I've seen other projects using this data, and am optimistic that the use will be ok. USFS: The US Forest Service is the US governmental body that officially stewards the Pacific Crest Trail. As such, they keep an official centerline of the trail, but it is much less accurate than the Halfmile or OpenStreetMap centerlines. The PCT USFS page is here: www.fs.usda.gov/pct/ . GPSTracks: On my hike of the PCT in 2019, I recorded my own GPS data, generally at 5-second intervals. This raw data has been copied into data/raw/tracks . While it's generally not accurate enough to use as an official centerline for an app, it will be helpful to use to geocode photos, and thus fill in waypoints that are missing from open data sources. Wilderness Boundaries: Wilderness boundaries are retrieved from wilderness.net . National Park Boundaries: National Park boundaries are retrieved from the NPS open data portal . National Forest Boundaries: National Forest boundaries are retrieved from the USFS website , under the heading Administrative Forest Boundaries . State Boundaries: State boundaries from the Census' TIGER dataset . Cell Towers: Cell tower data come from OpenCellID . Ideally at some point I'll implement a simple line-of-sight algorithm and then calculate where on the trail has cell service. Lightning Counts: daily lightning counts for 0.1-degree bins are available since ~1986 from NOAA . Note that the raw data of where every lightning strike hits is closed source and must be purchased, but a NOAA contract lets daily extracts be made public. Transit: I get transit data from the Transitland database. This is a bit easier than working with raw GTFS (General Transit Feed Specification) data, and they've done a bit of work to deduplicate data and connect the same stops in different data extracts from different providers. National Elevation Dataset: In the US, the most accurate elevation data comes from the USGS's 3D Elevation Program . They have a seamless Digital Elevation Model (DEM) at \u2153 arc-second resolution, which is about 10 meters. USGS Hydrography: The USGS's National Hydrography products are the premier water source datasets for the US. The Watershed Boundary dataset splits the US into a pyramid of smaller and smaller hydrologic regions. I first use the Watershed Boundary dataset to find the watersheds that the PCT passes through, then go to the National Hydrography dataset to find all streams, lakes, and springs near the trail. PCT Water Report : The PCT water report is an openly-licensed set of spreadsheets with reports from hikers of which water sources are flowing. EPA AirNow: The EPA has an API where you can access current air quality regions. NIFC: National Interagency Fire Center. GeoMAC is closing as of the end of April 2020, and NIFC is the new place for retrieving wildfire boundaries. CalFire Recreation.gov: Recreation.gov has an API for accessing information about features in National Forests. Repository Structure \u00b6 data_source/ : This folder contains wrappers for each individual data source. Files are generally named by the organization that releases the data I use, and there can be more than one loader in each file. These classes attempt to abstract reading of the original data, though the classes do not all have the same interface. These should hold only function/class definitions, and no code should be evaluated when the script is run. geom.py : This file holds geometric abstractions. Included are functions to reproject data between CRS's, truncate precision of geometries, create buffers at a given distance around a geometry, and project 3D coordinates onto the 2D plane. Again, this file should only define functions and constants, and not evaluate anything itself. grid.py : Helpers for generating intersections with regularly spaced grids. For example, USGS elevation files, or topo quads, are packaged for download in a regular grid, and this helps to find which files intersect a provided geometry. Note that for USGS data, it's probably easier to just use the USGS National Map API. main.py : This should handle delegating commands to other files. I.e. the only file that should be run directly from the command line. parse.py : This is a wrapper for uploading data to my Parse Server instance. It wraps the Parse REST API to upload Parse's custom classes, like GeoPoint s. Also in this file (?) is where schema-checking will take place, making sure data uploads conform to the JSON schemas defined here . s3.py : Wrapper for the AWS S3 CLI. This doesn't use boto3 directly, because I already knew the CLI commands I wanted to use, and didn't want to spend the time figuring out boto3. tiles.py : This holds functions to make working with tiled data easier. Like supplying a Polygon and getting the XYZ or TMS tile coordinates . trail.py : This holds the meat of taking the data sources and assembling them into a useful dataset. util.py : Small non-geometric utilities Auto-updating layers \u00b6 There are a few layers that are designed to auto-update: National Weather Service forecasts EPA AirNow air quality polygons National Interagency Fire Center (NIFC) current wildfire polygons The first is handled in a separate repository: nst-guide/ndfd_current . The second two are defined within this repository. All three are designed to be run with AWS Lambda. AWS Lambda \u00b6 From Wikipedia: AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code. Basically, AWS Lambda is a (somewhat) easy, very cheap way to run small pieces of code regularly or in response to events. For all of my use cases above, I can set AWS Lambda to run every few hours. In my testing of my code to update wildfire perimeters from NIFC, it used 244 MB of memory and took 8178.56 ms to run. From the AWS Lambda pricing page , it's roughly $0.0000005 for each 100ms when 320MB of memory is allocated. That means the 8100ms job cost $0.0000405, and running it every 4 hours would cost $0.00729 every 30 days. Aka basically free. AWS Lambda Downsides \u00b6 The biggest downsides of AWS Lambda by far are the hard limits on how big your Deployment Package (aka all unzipped code, including all dependencies) can be: 250MB . When some of your dependencies are GDAL, GeoPandas -> pandas -> numpy, that limit is really easy to surpass. I've been unable to include either GeoPandas or Fiona (on their own, not together) inside the deployment package. This means that rewriting code to avoid dependencies on any large library, such as GeoPandas and Fiona, is inevitable. AWS Lambda Dependencies \u00b6 You can't just pip install gdal , or use conda , or even use Docker in AWS Lambda. All your code and dependencies must be pre-built and provided as a Zip file. Luckily, AWS Lambda has the concept of Lambda Layers . These are code packages created by you or by others that you can point to and use, without having to package the code yourself. So you can include a layer for, say, GDAL , and then run code that depends on GDAL libraries without an issue. You can only use a maximum of five with any given function. In practice, this shouldn't be as bad as it sounds because you could zip multiple dependencies into a single layer. The bigger problem in practice is hitting Lambda's 250MB hard limit on repository size. To use a layer, the easiest way is to include its Amazon Resource Number (ARN). So, for example, to include geolambda , I can go to {my function} > Layers > Add a layer > Provide a layer version ARN and paste arn:aws:lambda:us-east-1:552188055668:layer:geolambda:4 the unique identifier for that specific version of geolambda that I use. Note that layers are specific to an AWS region, so the layer referenced by this specific ARN will only work in US-East-1. Sometimes layers will be pre-built in multiple regions. For example, geolambda is pre-built in US-East-1, US-West-2, and EU-Central-1. To use the layer in any other AWS region, you'd have to build and upload the layer yourself to that region. I use several layers: geolambda and geolambda-python . These are immensely helpful layers that provide geospatial libraries within Lambda. geolambda provides PROJ.5, GEOS, GeoTIFF, HDF\u2158, SZIP, NetCDF, OpenJPEG, WEBP, ZSTD, and GDAL. geolambda-python additionally provides GDAL (the Python bindings), rasterio, shapely, pyproj, and numpy. Note that if you want to use the Python bindings, you must provide both , not just geolambda-python . It is possible to build both geolambda and geolambda-python layers yourself. Their Github READMEs have pretty good documentation, and I was able to build geolambda-python myself (as you would need to if you wanted to modify the package list.) Klayers-python37-requests . Klayers is a project to provide many common Python libraries as lambda layers. This is just an easy way to access requests , though in this case it would be easy to build yourself. And a couple packaged by me: nst-guide-fastkml-python37 : provides the fastkml package nst-guide-geojson-python37 : provides the geojson package nst-guide-pyshp-python37 : provides the pyshp package. Because Fiona was too big to be packaged on AWS Lambda, and I needed to read Shapefiles for the wildfire perimeters updating, I had to find an alternative. Luckily, pyshp is able to read shapefiles and is only ~200KB of code. Packaging dependencies for AWS Lambda \u00b6 As mentioned above, sometimes you'll need to build lambda layers yourself. This article was pretty helpful. The general gist is: mkdir -p layer/python pip install {package_list} -t layer/python cd layer zip -r aws-layer.zip python Then go to the AWS Lambda console, choose Layers from the side pane, and upload the Zip archive as a new layer. Python packages must be within a python/ directory in the Zip archive. You won't be able to load the library without the top-level python directory. Also, if the Python packages have any (or depend on any) native code, you should run the above packaging steps on a Linux machine, so that the layer will work on Amazon's linux-based architecture. For pure-Python packages, you should be able build the Zip archive on any OS. AWS Lambda IAM Role \u00b6 Each of my AWS Lambda functions upload files to AWS S3 in order to be served to users. This means that the function must be associated with a valid IAM role to be permitted to access and modify files in my S3 bucket. Note that in order to use the ACL='public-read' option, the IAM role running the Lambda function must also have the S3:putObjectAcl permission .","title":"Home"},{"location":"#nst-guide-data","text":"","title":"NST Guide Data"},{"location":"#overview","text":"This repository contains code for data pipelines to generate map waypoints and layers of interest from open map data sources.","title":"Overview"},{"location":"#data-sources","text":"Town Boundaries: for now, these are drawn by hand using local trail knowledge and geojson.io and saved to data/pct/polygon/bound/town/{ca,or,wa}/*.geojson . OpenStreetMap : I use OSM for trail information and town waypoints. Initially, I planned to download whole-state extracts from Geofabrik . After discovering the osmnx package for Python, I decided to use that instead. That calls OSM's Overpass API , and then helpfully manages the result in a graph. This has a few benefits: No need to download any large files. Using the Geofabrik extracts, California is nearly 1GB of compressed data, and most of that is far from the trail, and really not necessary for this project. Speed. Unsurprisingly, when you're working with 1GB of compressed data just for california, computations aren't going to be super fast. Faster updates. Geofabrik extracts are updated around once a week I think, while the Overpass API has near-instant updating. That means that if I fix an issue with the data in OSM's editor, then I can get working with the new data immediately. Halfmile : Halfmile has accurate route information and a few thousand waypoints for the trail. I have yet to hear final confirmation that this is openly licensed, but I've seen other projects using this data, and am optimistic that the use will be ok. USFS: The US Forest Service is the US governmental body that officially stewards the Pacific Crest Trail. As such, they keep an official centerline of the trail, but it is much less accurate than the Halfmile or OpenStreetMap centerlines. The PCT USFS page is here: www.fs.usda.gov/pct/ . GPSTracks: On my hike of the PCT in 2019, I recorded my own GPS data, generally at 5-second intervals. This raw data has been copied into data/raw/tracks . While it's generally not accurate enough to use as an official centerline for an app, it will be helpful to use to geocode photos, and thus fill in waypoints that are missing from open data sources. Wilderness Boundaries: Wilderness boundaries are retrieved from wilderness.net . National Park Boundaries: National Park boundaries are retrieved from the NPS open data portal . National Forest Boundaries: National Forest boundaries are retrieved from the USFS website , under the heading Administrative Forest Boundaries . State Boundaries: State boundaries from the Census' TIGER dataset . Cell Towers: Cell tower data come from OpenCellID . Ideally at some point I'll implement a simple line-of-sight algorithm and then calculate where on the trail has cell service. Lightning Counts: daily lightning counts for 0.1-degree bins are available since ~1986 from NOAA . Note that the raw data of where every lightning strike hits is closed source and must be purchased, but a NOAA contract lets daily extracts be made public. Transit: I get transit data from the Transitland database. This is a bit easier than working with raw GTFS (General Transit Feed Specification) data, and they've done a bit of work to deduplicate data and connect the same stops in different data extracts from different providers. National Elevation Dataset: In the US, the most accurate elevation data comes from the USGS's 3D Elevation Program . They have a seamless Digital Elevation Model (DEM) at \u2153 arc-second resolution, which is about 10 meters. USGS Hydrography: The USGS's National Hydrography products are the premier water source datasets for the US. The Watershed Boundary dataset splits the US into a pyramid of smaller and smaller hydrologic regions. I first use the Watershed Boundary dataset to find the watersheds that the PCT passes through, then go to the National Hydrography dataset to find all streams, lakes, and springs near the trail. PCT Water Report : The PCT water report is an openly-licensed set of spreadsheets with reports from hikers of which water sources are flowing. EPA AirNow: The EPA has an API where you can access current air quality regions. NIFC: National Interagency Fire Center. GeoMAC is closing as of the end of April 2020, and NIFC is the new place for retrieving wildfire boundaries. CalFire Recreation.gov: Recreation.gov has an API for accessing information about features in National Forests.","title":"Data Sources"},{"location":"#repository-structure","text":"data_source/ : This folder contains wrappers for each individual data source. Files are generally named by the organization that releases the data I use, and there can be more than one loader in each file. These classes attempt to abstract reading of the original data, though the classes do not all have the same interface. These should hold only function/class definitions, and no code should be evaluated when the script is run. geom.py : This file holds geometric abstractions. Included are functions to reproject data between CRS's, truncate precision of geometries, create buffers at a given distance around a geometry, and project 3D coordinates onto the 2D plane. Again, this file should only define functions and constants, and not evaluate anything itself. grid.py : Helpers for generating intersections with regularly spaced grids. For example, USGS elevation files, or topo quads, are packaged for download in a regular grid, and this helps to find which files intersect a provided geometry. Note that for USGS data, it's probably easier to just use the USGS National Map API. main.py : This should handle delegating commands to other files. I.e. the only file that should be run directly from the command line. parse.py : This is a wrapper for uploading data to my Parse Server instance. It wraps the Parse REST API to upload Parse's custom classes, like GeoPoint s. Also in this file (?) is where schema-checking will take place, making sure data uploads conform to the JSON schemas defined here . s3.py : Wrapper for the AWS S3 CLI. This doesn't use boto3 directly, because I already knew the CLI commands I wanted to use, and didn't want to spend the time figuring out boto3. tiles.py : This holds functions to make working with tiled data easier. Like supplying a Polygon and getting the XYZ or TMS tile coordinates . trail.py : This holds the meat of taking the data sources and assembling them into a useful dataset. util.py : Small non-geometric utilities","title":"Repository Structure"},{"location":"#auto-updating-layers","text":"There are a few layers that are designed to auto-update: National Weather Service forecasts EPA AirNow air quality polygons National Interagency Fire Center (NIFC) current wildfire polygons The first is handled in a separate repository: nst-guide/ndfd_current . The second two are defined within this repository. All three are designed to be run with AWS Lambda.","title":"Auto-updating layers"},{"location":"#aws-lambda","text":"From Wikipedia: AWS Lambda is an event-driven, serverless computing platform provided by Amazon as a part of Amazon Web Services. It is a computing service that runs code in response to events and automatically manages the computing resources required by that code. Basically, AWS Lambda is a (somewhat) easy, very cheap way to run small pieces of code regularly or in response to events. For all of my use cases above, I can set AWS Lambda to run every few hours. In my testing of my code to update wildfire perimeters from NIFC, it used 244 MB of memory and took 8178.56 ms to run. From the AWS Lambda pricing page , it's roughly $0.0000005 for each 100ms when 320MB of memory is allocated. That means the 8100ms job cost $0.0000405, and running it every 4 hours would cost $0.00729 every 30 days. Aka basically free.","title":"AWS Lambda"},{"location":"#aws-lambda-downsides","text":"The biggest downsides of AWS Lambda by far are the hard limits on how big your Deployment Package (aka all unzipped code, including all dependencies) can be: 250MB . When some of your dependencies are GDAL, GeoPandas -> pandas -> numpy, that limit is really easy to surpass. I've been unable to include either GeoPandas or Fiona (on their own, not together) inside the deployment package. This means that rewriting code to avoid dependencies on any large library, such as GeoPandas and Fiona, is inevitable.","title":"AWS Lambda Downsides"},{"location":"#aws-lambda-dependencies","text":"You can't just pip install gdal , or use conda , or even use Docker in AWS Lambda. All your code and dependencies must be pre-built and provided as a Zip file. Luckily, AWS Lambda has the concept of Lambda Layers . These are code packages created by you or by others that you can point to and use, without having to package the code yourself. So you can include a layer for, say, GDAL , and then run code that depends on GDAL libraries without an issue. You can only use a maximum of five with any given function. In practice, this shouldn't be as bad as it sounds because you could zip multiple dependencies into a single layer. The bigger problem in practice is hitting Lambda's 250MB hard limit on repository size. To use a layer, the easiest way is to include its Amazon Resource Number (ARN). So, for example, to include geolambda , I can go to {my function} > Layers > Add a layer > Provide a layer version ARN and paste arn:aws:lambda:us-east-1:552188055668:layer:geolambda:4 the unique identifier for that specific version of geolambda that I use. Note that layers are specific to an AWS region, so the layer referenced by this specific ARN will only work in US-East-1. Sometimes layers will be pre-built in multiple regions. For example, geolambda is pre-built in US-East-1, US-West-2, and EU-Central-1. To use the layer in any other AWS region, you'd have to build and upload the layer yourself to that region. I use several layers: geolambda and geolambda-python . These are immensely helpful layers that provide geospatial libraries within Lambda. geolambda provides PROJ.5, GEOS, GeoTIFF, HDF\u2158, SZIP, NetCDF, OpenJPEG, WEBP, ZSTD, and GDAL. geolambda-python additionally provides GDAL (the Python bindings), rasterio, shapely, pyproj, and numpy. Note that if you want to use the Python bindings, you must provide both , not just geolambda-python . It is possible to build both geolambda and geolambda-python layers yourself. Their Github READMEs have pretty good documentation, and I was able to build geolambda-python myself (as you would need to if you wanted to modify the package list.) Klayers-python37-requests . Klayers is a project to provide many common Python libraries as lambda layers. This is just an easy way to access requests , though in this case it would be easy to build yourself. And a couple packaged by me: nst-guide-fastkml-python37 : provides the fastkml package nst-guide-geojson-python37 : provides the geojson package nst-guide-pyshp-python37 : provides the pyshp package. Because Fiona was too big to be packaged on AWS Lambda, and I needed to read Shapefiles for the wildfire perimeters updating, I had to find an alternative. Luckily, pyshp is able to read shapefiles and is only ~200KB of code.","title":"AWS Lambda Dependencies"},{"location":"#packaging-dependencies-for-aws-lambda","text":"As mentioned above, sometimes you'll need to build lambda layers yourself. This article was pretty helpful. The general gist is: mkdir -p layer/python pip install {package_list} -t layer/python cd layer zip -r aws-layer.zip python Then go to the AWS Lambda console, choose Layers from the side pane, and upload the Zip archive as a new layer. Python packages must be within a python/ directory in the Zip archive. You won't be able to load the library without the top-level python directory. Also, if the Python packages have any (or depend on any) native code, you should run the above packaging steps on a Linux machine, so that the layer will work on Amazon's linux-based architecture. For pure-Python packages, you should be able build the Zip archive on any OS.","title":"Packaging dependencies for AWS Lambda"},{"location":"#aws-lambda-iam-role","text":"Each of my AWS Lambda functions upload files to AWS S3 in order to be served to users. This means that the function must be associated with a valid IAM role to be permitted to access and modify files in my S3 bucket. Note that in order to use the ACL='public-read' option, the IAM role running the Lambda function must also have the S3:putObjectAcl permission .","title":"AWS Lambda IAM Role"},{"location":"cli/api/export/","text":"","title":"Export"},{"location":"cli/api/photos/","text":"geotag_photos \u00b6 Geotag photos from Photos.app. > python code/main.py photos geotag-photos --help Usage: main.py photos geotag-photos [OPTIONS] Geotag photos from album using watch's GPS tracks Options: -a, --album TEXT Photos.app album to use for photos geocoding. --exif Include metadata from exiftool --all-cols Don't select minimal columns -o, --out-path FILE Output path for photo metadata GeoJSON file [required] -s, --start-date TEXT Start date to find photos -e, --end-date TEXT End date to find photos -x, --xw-path FILE Output path for UUID-photo path crosswalk --help Show this message and exit. ``` ### Example ```bash # Package's entry point python code/main.py \\ `# photos command` \\ photos \\ `# geotag-photos subcommand` \\ geotag-photos \\ `# Select photos from album named nst-guide-web` \\ -a nst-guide-web \\ `# Output the main JSON file with photo metadata to the path` \\ `# nst-guide-web-photos.geojson` \\ -o nst-guide-web-photos.geojson \\ `# Output filename crosswalk to photos_xw.json` \\ -x photos_xw.json copy_using_xw \u00b6 > python code/main.py photos copy-using-xw --help Usage: main.py photos copy-using-xw [OPTIONS] FILE Copy files to out_dir using JSON crosswalk For any non-JPEG files, this calls `sips` (mac-cli) to convert them to JPEG. Options: -o, --out-dir FILE Output directory for copied photos [required] --help Show this message and exit. # Package entry point python code/main.py \\ ` # photos command` \\ photos \\ ` # copy-using-xw subcommand` \\ copy-using-xw \\ ` # Copy photos to directory tmp` \\ -o tmp \\ ` # use photos_xw.json for copying photos` \\ photos_xw.json","title":"Photos"},{"location":"cli/api/photos/#geotag_photos","text":"Geotag photos from Photos.app. > python code/main.py photos geotag-photos --help Usage: main.py photos geotag-photos [OPTIONS] Geotag photos from album using watch's GPS tracks Options: -a, --album TEXT Photos.app album to use for photos geocoding. --exif Include metadata from exiftool --all-cols Don't select minimal columns -o, --out-path FILE Output path for photo metadata GeoJSON file [required] -s, --start-date TEXT Start date to find photos -e, --end-date TEXT End date to find photos -x, --xw-path FILE Output path for UUID-photo path crosswalk --help Show this message and exit. ``` ### Example ```bash # Package's entry point python code/main.py \\ `# photos command` \\ photos \\ `# geotag-photos subcommand` \\ geotag-photos \\ `# Select photos from album named nst-guide-web` \\ -a nst-guide-web \\ `# Output the main JSON file with photo metadata to the path` \\ `# nst-guide-web-photos.geojson` \\ -o nst-guide-web-photos.geojson \\ `# Output filename crosswalk to photos_xw.json` \\ -x photos_xw.json","title":"geotag_photos"},{"location":"cli/api/photos/#copy_using_xw","text":"> python code/main.py photos copy-using-xw --help Usage: main.py photos copy-using-xw [OPTIONS] FILE Copy files to out_dir using JSON crosswalk For any non-JPEG files, this calls `sips` (mac-cli) to convert them to JPEG. Options: -o, --out-dir FILE Output directory for copied photos [required] --help Show this message and exit. # Package entry point python code/main.py \\ ` # photos command` \\ photos \\ ` # copy-using-xw subcommand` \\ copy-using-xw \\ ` # Copy photos to directory tmp` \\ -o tmp \\ ` # use photos_xw.json for copying photos` \\ photos_xw.json","title":"copy_using_xw"},{"location":"cli/api/tiles/","text":"package_tiles \u00b6 Note : this is deprecated in favor of Mapbox's sideloading offline maps . A command line interface to package map tiles into Zip files for given geometries. Map tiles, especially vector map tiles, are tiny, on the order of a couple dozen kb. It's a waste of time spent on HTTP pinging to download each of them individually for offline use, especially when I know ahead of time the areas a user will download. So the idea with this is that I create zipped files that contain all the map tiles for a given section of trail. Then the mobile app downloads that zip file, the app extracts the tiles, I point Mapbox to the extracted folders, and voil\u00e0! offline maps. The command takes a geometry, optionally generates a buffer, and copies the map tiles within that buffer to an output directory. (This command doesn't actually zip the folder, so that inspection is easy.) The output folder structure is output_dir/{buffer_distance}/{tileset_name}/{z}/{x}/{y}.{png,pbf} with the following definitions: buffer_distance : distance around geometry in miles, provided as --buffer option. If multiple buffers are provided, tiles will be created as the difference between the current buffer distance and the previous one. I.e. if you pass --buffer \"2 5 10\" , the 5 directory will hold the tiles that are in the 5-mile buffer but outside the 2-mile buffer. If you don't want this nesting, just run the command multiple times, each time specifying a single buffer value. tileset_name : this name is derived from the last name of the provided directory path. So if the directory path is path/to/dir , the tileset name will be set to dir z , x , y : this corresponds to the coordinate of the tile in either XYZ or TMS coordinates. This command does not convert TMS tiles to XYZ. If the tile source is in TMS, the destination source will be as well. png , pbf : the extension of the output tiles is the same as the source tiles. API \u00b6 > python main.py package-tiles --help Usage: main.py package-tiles [OPTIONS] Package tiles into directory based on distance from trail Example: python main.py package-tiles -g ../data/pct/polygon/bound/town/ca/acton.geojson -b \"0 1 2\" -d ~/Desktop -o out/ Options: -g, --geometry FILE Geometries to use for packaging tiles. Can be any format readable by GeoPandas. [required] -b, --buffer TEXT Buffer distance (in miles) to use around provided geometries. If you want multiple buffer distances, pass as --buffer \"2 5 10\" [required] -d, --directory DIRECTORY Directory root of tiles to package. If multiple options are provided, will package each of them. [required] -t, --tile-json FILE Paths to tile.json files for each directory. If not provided, assumes a tile JSON file is at directory/tile.json. Otherwise, the same number of options as directory must be provided. -z, --min-zoom INTEGER Min zoom for each tile source -Z, --max-zoom INTEGER Max zoom for each tile source -o, --output PATH Output directory [required] --raise / --no-raise Whether to raise an error if a desired tile is not found in the directory. -v, --verbose Verbose output --help Show this message and exit. Example \u00b6 Here, the first source tile pair links to a directory with OpenMapTiles tiles, and copies a maximum of zoom 14 (inclusive). The next copies Terrain RGB png files up to zoom 12 (inclusive); the last copies contours up to zoom 11 (inclusive). The tiles are copy to output_dir/ (relative to the current directory). --no-raise tells it not to raise an error if a requested tile does not exist. For example, I didn't download/generate OpenMapTiles for Mexico or Canada, so part of the buffer for the start and end of the trail might be missing. python main.py package-tiles \\ --geometry ../data/pct/line/halfmile/CA_Sec_A_tracks.geojson \\ --buffer \"2 5 10\" \\ -d ../../openmaptiles/ca_or_wa \\ -Z 14 \\ -d ../../hillshade/data/terrain_png \\ -Z 12 \\ -d ../../contours/contours \\ -Z 11 \\ -o output_dir/ \\ --no-raise \\ --verbose","title":"Tiles"},{"location":"cli/api/tiles/#package_tiles","text":"Note : this is deprecated in favor of Mapbox's sideloading offline maps . A command line interface to package map tiles into Zip files for given geometries. Map tiles, especially vector map tiles, are tiny, on the order of a couple dozen kb. It's a waste of time spent on HTTP pinging to download each of them individually for offline use, especially when I know ahead of time the areas a user will download. So the idea with this is that I create zipped files that contain all the map tiles for a given section of trail. Then the mobile app downloads that zip file, the app extracts the tiles, I point Mapbox to the extracted folders, and voil\u00e0! offline maps. The command takes a geometry, optionally generates a buffer, and copies the map tiles within that buffer to an output directory. (This command doesn't actually zip the folder, so that inspection is easy.) The output folder structure is output_dir/{buffer_distance}/{tileset_name}/{z}/{x}/{y}.{png,pbf} with the following definitions: buffer_distance : distance around geometry in miles, provided as --buffer option. If multiple buffers are provided, tiles will be created as the difference between the current buffer distance and the previous one. I.e. if you pass --buffer \"2 5 10\" , the 5 directory will hold the tiles that are in the 5-mile buffer but outside the 2-mile buffer. If you don't want this nesting, just run the command multiple times, each time specifying a single buffer value. tileset_name : this name is derived from the last name of the provided directory path. So if the directory path is path/to/dir , the tileset name will be set to dir z , x , y : this corresponds to the coordinate of the tile in either XYZ or TMS coordinates. This command does not convert TMS tiles to XYZ. If the tile source is in TMS, the destination source will be as well. png , pbf : the extension of the output tiles is the same as the source tiles.","title":"package_tiles"},{"location":"cli/api/tiles/#api","text":"> python main.py package-tiles --help Usage: main.py package-tiles [OPTIONS] Package tiles into directory based on distance from trail Example: python main.py package-tiles -g ../data/pct/polygon/bound/town/ca/acton.geojson -b \"0 1 2\" -d ~/Desktop -o out/ Options: -g, --geometry FILE Geometries to use for packaging tiles. Can be any format readable by GeoPandas. [required] -b, --buffer TEXT Buffer distance (in miles) to use around provided geometries. If you want multiple buffer distances, pass as --buffer \"2 5 10\" [required] -d, --directory DIRECTORY Directory root of tiles to package. If multiple options are provided, will package each of them. [required] -t, --tile-json FILE Paths to tile.json files for each directory. If not provided, assumes a tile JSON file is at directory/tile.json. Otherwise, the same number of options as directory must be provided. -z, --min-zoom INTEGER Min zoom for each tile source -Z, --max-zoom INTEGER Max zoom for each tile source -o, --output PATH Output directory [required] --raise / --no-raise Whether to raise an error if a desired tile is not found in the directory. -v, --verbose Verbose output --help Show this message and exit.","title":"API"},{"location":"cli/api/tiles/#example","text":"Here, the first source tile pair links to a directory with OpenMapTiles tiles, and copies a maximum of zoom 14 (inclusive). The next copies Terrain RGB png files up to zoom 12 (inclusive); the last copies contours up to zoom 11 (inclusive). The tiles are copy to output_dir/ (relative to the current directory). --no-raise tells it not to raise an error if a requested tile does not exist. For example, I didn't download/generate OpenMapTiles for Mexico or Canada, so part of the buffer for the start and end of the trail might be missing. python main.py package-tiles \\ --geometry ../data/pct/line/halfmile/CA_Sec_A_tracks.geojson \\ --buffer \"2 5 10\" \\ -d ../../openmaptiles/ca_or_wa \\ -Z 14 \\ -d ../../hillshade/data/terrain_png \\ -Z 12 \\ -d ../../contours/contours \\ -Z 11 \\ -o output_dir/ \\ --no-raise \\ --verbose","title":"Example"},{"location":"cli/quickstart/","text":"Quickstart \u00b6 This folder has overviews of the available CLI commands. Essentially, if it's a standard process to create and upload certain data to S3 for the website or mobile app, I'll try to document the sequence of commands in a file here. For full options, check the API for each command, which is also available by appending --help to the command.","title":"Overview"},{"location":"cli/quickstart/#quickstart","text":"This folder has overviews of the available CLI commands. Essentially, if it's a standard process to create and upload certain data to S3 for the website or mobile app, I'll try to document the sequence of commands in a file here. For full options, check the API for each command, which is also available by appending --help to the command.","title":"Quickstart"},{"location":"cli/quickstart/national_forests/","text":"National Forests layer \u00b6 Properties: forestname : Name of forest from source USFS GIS dataset gis_acres : # of acres of forest from source USFS GIS dataset geometry : Polygon/MultiPolygon geometry of forest from source USFS GIS dataset. Only polygons that intersect the trail of interest are included, but some national forests, like Inyo National Forest, are quite wide-ranging, so geometries are kept in Nevada I think because it has the same name. length : length of trail in national forest in meters wiki_image : url to Wikipedia image. I try to select the best image but it can be difficult sometimes. wiki_url : url to wikipedia page wiki_summary : summary of wikipedia page. Usually this is the first paragraph. official_url : url to National Forest homepage # Make temp directory mkdir -p tmp # Generate national forest polygons python code/main.py export national-forests \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/nationalforests.geojson # Generate point labels from those polygons python code/main.py geom polylabel \\ ` # include only the forestname attribute` \\ -y forestname \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/nationalforests.geojson > tmp/nationalforests_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/nationalforests_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/nationalforests_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/nationalforests.geojson\", \"layer\":\"nationalforests\"}' \\ -L '{\"file\":\"tmp/nationalforests_label.geojson\", \"layer\":\"nationalforests_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'National Forests' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nps.gov/\" target=\"_blank\">\u00a9 USFS</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/nationalforest/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/nationalforests.json \\ ` # input JSON file` \\ tmp/nationalforests_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/nationalforests_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/nationalforest/ Add new vector tiles aws s3 cp \\ tmp/nationalforests_tiles s3://tiles.nst.guide/pct/nationalforest/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalforests.json s3://tiles.nst.guide/pct/nationalforest/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"National Forests"},{"location":"cli/quickstart/national_forests/#national-forests-layer","text":"Properties: forestname : Name of forest from source USFS GIS dataset gis_acres : # of acres of forest from source USFS GIS dataset geometry : Polygon/MultiPolygon geometry of forest from source USFS GIS dataset. Only polygons that intersect the trail of interest are included, but some national forests, like Inyo National Forest, are quite wide-ranging, so geometries are kept in Nevada I think because it has the same name. length : length of trail in national forest in meters wiki_image : url to Wikipedia image. I try to select the best image but it can be difficult sometimes. wiki_url : url to wikipedia page wiki_summary : summary of wikipedia page. Usually this is the first paragraph. official_url : url to National Forest homepage # Make temp directory mkdir -p tmp # Generate national forest polygons python code/main.py export national-forests \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/nationalforests.geojson # Generate point labels from those polygons python code/main.py geom polylabel \\ ` # include only the forestname attribute` \\ -y forestname \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/nationalforests.geojson > tmp/nationalforests_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/nationalforests_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/nationalforests_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/nationalforests.geojson\", \"layer\":\"nationalforests\"}' \\ -L '{\"file\":\"tmp/nationalforests_label.geojson\", \"layer\":\"nationalforests_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'National Forests' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nps.gov/\" target=\"_blank\">\u00a9 USFS</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/nationalforest/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/nationalforests.json \\ ` # input JSON file` \\ tmp/nationalforests_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/nationalforests_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/nationalforest/ Add new vector tiles aws s3 cp \\ tmp/nationalforests_tiles s3://tiles.nst.guide/pct/nationalforest/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalforests.json s3://tiles.nst.guide/pct/nationalforest/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"National Forests layer"},{"location":"cli/quickstart/national_parks/","text":"National Parks layer \u00b6 Properties: description : NPS description of park. A couple sentences directionsInfo : NPS directions to park. directionsUrl : URL to NPS website for more info on directions fullName : full name of park, i.e. \"Devils Postpile National Monument\" length : length in meters of PCT in park url : URL to NPS webpage for park weatherInfo : NPS weather info # Make temp directory mkdir -p tmp # Generate national park polygons python code/main.py export national-parks \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/nationalparks.geojson # Generate national park labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y fullName \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/nationalparks.geojson > tmp/nationalparks_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/nationalparks_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/nationalparks_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/nationalparks.geojson\", \"layer\":\"nationalparks\"}' \\ -L '{\"file\":\"tmp/nationalparks_label.geojson\", \"layer\":\"nationalparks_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'National Parks' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nps.gov/\" target=\"_blank\">\u00a9 NPS</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/nationalpark/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/nationalparks.json \\ ` # input JSON file` \\ tmp/nationalparks_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/nationalparks_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/nationalpark/ Add new vector tiles aws s3 cp \\ tmp/nationalparks_tiles s3://tiles.nst.guide/nationalpark/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalparks.json s3://tiles.nst.guide/nationalpark/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"National Parks"},{"location":"cli/quickstart/national_parks/#national-parks-layer","text":"Properties: description : NPS description of park. A couple sentences directionsInfo : NPS directions to park. directionsUrl : URL to NPS website for more info on directions fullName : full name of park, i.e. \"Devils Postpile National Monument\" length : length in meters of PCT in park url : URL to NPS webpage for park weatherInfo : NPS weather info # Make temp directory mkdir -p tmp # Generate national park polygons python code/main.py export national-parks \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/nationalparks.geojson # Generate national park labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y fullName \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/nationalparks.geojson > tmp/nationalparks_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/nationalparks_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/nationalparks_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/nationalparks.geojson\", \"layer\":\"nationalparks\"}' \\ -L '{\"file\":\"tmp/nationalparks_label.geojson\", \"layer\":\"nationalparks_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'National Parks' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nps.gov/\" target=\"_blank\">\u00a9 NPS</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/nationalpark/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/nationalparks.json \\ ` # input JSON file` \\ tmp/nationalparks_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/nationalparks_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/nationalpark/ Add new vector tiles aws s3 cp \\ tmp/nationalparks_tiles s3://tiles.nst.guide/nationalpark/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalparks.json s3://tiles.nst.guide/nationalpark/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"National Parks layer"},{"location":"cli/quickstart/tiles/","text":"Tiles \u00b6 Uploading newer versions of tiles near the PCT to AWS \u00b6 Since I'm trying to have an app for the PCT specifically, I care about having updated OpenStreetMap data near the PCT, but further away from the trail and from trail towns is not as important. It's simple to update tiles for any arbitrary Geofabrik extract region, aka easy to update for the state of Washington, as I can just run OpenMapTiles for that region. The drawback of that, however, is that map tiles as a directory are thousands and thousands of very tiny files, so the PUT requests to AWS S3 actually add up. For example, for zooms 0-14, the state of Oregon makes up 191306 tiles. Since it's $0.005 per 1000 put requests, it's just shy of $1 each time I update the tiles. I calculated that if I were to upload tiles for the entire continental US, it would likely be just shy of $100 for each set of PUT requests. The following instructions show how to update tiles for just a given buffer around the PCT, which is on the order of $0.07 for each upload for each fifth of the trail. Create new tiles from e.g. the OpenMapTiles repository . Export the .mbtiles file to a directory of tiles: mb-util tiles.mbtiles tiles --image_format = pbf Get tiles for a section of the PCT, e.g. -t : trail code -s : trail section (optional) -z : min zoom -Z : max zoom -b : buffer distance in miles around trail python code/main.py tiles tiles-for-trail \\ -t pct \\ -s ca_south \\ -z 0 \\ -Z 14 \\ -b 15 > tiles_ca_south.txt Loop over those tiles and copy them to a new directory It's really slow to run aws s3 cp a new time for each file, and much much faster to run aws s3 cp --recursive on a directory, so the best approach is to copy the desired tiles into a new directory, then aws s3 cp that directory to AWS. Remove the [ , , , and ] characters, and reorder x,y,z to z,x,y : fname = \"tiles_ca_south.txt\" # Make sure to update .pbf to .png if you're working with non-vector tiles tiles = $( cat $fname | \\ tr -d '[,]' | \\ awk '{print $3 \"/\" $1 \"/\" $2 \".pbf\"}' ) Outputs: > echo $tiles ... 14/6632/2892.pbf 14/6632/2893.pbf 14/6632/2894.pbf 14/6632/2895.pbf 14/6632/2896.pbf ... Then copy the tiles that exist into a new directory (note, you could probably make this faster by skipping the if $tile check, since cp will just print an error but not stop the loop. However, this might create empty directories if $tile doesn't actually exist.): new_dir = \"../tiles_tmp\" echo $tiles | while read tile ; do if [ -f $tile ] ; then echo \" $tile \" ; mkdir -p $new_dir / $( dirname $tile ) cp $tile $new_dir / $tile fi done Upload to S3 Apply one-week caching plus one-year stale while revalidate. aws s3 cp \\ $new_dir s3://tiles.nst.guide/openmaptiles/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ --cache-control \"public, max-age=604800, stale-while-revalidate=31536000\"","title":"Tiles"},{"location":"cli/quickstart/tiles/#tiles","text":"","title":"Tiles"},{"location":"cli/quickstart/tiles/#uploading-newer-versions-of-tiles-near-the-pct-to-aws","text":"Since I'm trying to have an app for the PCT specifically, I care about having updated OpenStreetMap data near the PCT, but further away from the trail and from trail towns is not as important. It's simple to update tiles for any arbitrary Geofabrik extract region, aka easy to update for the state of Washington, as I can just run OpenMapTiles for that region. The drawback of that, however, is that map tiles as a directory are thousands and thousands of very tiny files, so the PUT requests to AWS S3 actually add up. For example, for zooms 0-14, the state of Oregon makes up 191306 tiles. Since it's $0.005 per 1000 put requests, it's just shy of $1 each time I update the tiles. I calculated that if I were to upload tiles for the entire continental US, it would likely be just shy of $100 for each set of PUT requests. The following instructions show how to update tiles for just a given buffer around the PCT, which is on the order of $0.07 for each upload for each fifth of the trail. Create new tiles from e.g. the OpenMapTiles repository . Export the .mbtiles file to a directory of tiles: mb-util tiles.mbtiles tiles --image_format = pbf Get tiles for a section of the PCT, e.g. -t : trail code -s : trail section (optional) -z : min zoom -Z : max zoom -b : buffer distance in miles around trail python code/main.py tiles tiles-for-trail \\ -t pct \\ -s ca_south \\ -z 0 \\ -Z 14 \\ -b 15 > tiles_ca_south.txt Loop over those tiles and copy them to a new directory It's really slow to run aws s3 cp a new time for each file, and much much faster to run aws s3 cp --recursive on a directory, so the best approach is to copy the desired tiles into a new directory, then aws s3 cp that directory to AWS. Remove the [ , , , and ] characters, and reorder x,y,z to z,x,y : fname = \"tiles_ca_south.txt\" # Make sure to update .pbf to .png if you're working with non-vector tiles tiles = $( cat $fname | \\ tr -d '[,]' | \\ awk '{print $3 \"/\" $1 \"/\" $2 \".pbf\"}' ) Outputs: > echo $tiles ... 14/6632/2892.pbf 14/6632/2893.pbf 14/6632/2894.pbf 14/6632/2895.pbf 14/6632/2896.pbf ... Then copy the tiles that exist into a new directory (note, you could probably make this faster by skipping the if $tile check, since cp will just print an error but not stop the loop. However, this might create empty directories if $tile doesn't actually exist.): new_dir = \"../tiles_tmp\" echo $tiles | while read tile ; do if [ -f $tile ] ; then echo \" $tile \" ; mkdir -p $new_dir / $( dirname $tile ) cp $tile $new_dir / $tile fi done Upload to S3 Apply one-week caching plus one-year stale while revalidate. aws s3 cp \\ $new_dir s3://tiles.nst.guide/openmaptiles/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ --cache-control \"public, max-age=604800, stale-while-revalidate=31536000\"","title":"Uploading newer versions of tiles near the PCT to AWS"},{"location":"cli/quickstart/transit/","text":"Transit layer \u00b6 Properties: Stops: geometry : location of transit stops (usually Point ) tags : metadata taken straight from GTFS feed _trail : True if the stop was generated from the trail buffer pass _town : True if the stop was generated from the town polygon pass _nearby_stop : True if the stop is within the trail or town buffer, and not another stop on a route Routes: geometry : location of transit routes (usually LineString or MultiLineString ) tags : metadata taken straight from GTFS feed name : name of transit route vehicle_type : type of vehicle used for transit route. Usually bus color : Route color in 6 hexadecimal characters operated_by_name : Name of operator of route _trail : True if the route was generated from the trail buffer pass _town : True if the route was generated from the town polygon pass # Make temp directory mkdir -p tmp # Generate transit python code/main.py export transit \\ ` # trail code, i.e. 'pct'` \\ -t pct \\ ` # file to write transit routes to` \\ --out-routes tmp/transit_routes.geojson \\ ` # file to write transit stops to` \\ --out-stops tmp/transit_stops.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/transit_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/transit_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/transit_routes.geojson\", \"layer\":\"routes\"}' \\ -L '{\"file\":\"tmp/transit_stops.geojson\", \"layer\":\"stops\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Transit' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://transit.land/\" target=\"_blank\">\u00a9 Transitland</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/transit/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/transit.json \\ ` # input JSON file` \\ tmp/transit_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/transit_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/transit/ Add new vector tiles aws s3 cp \\ tmp/nationalparks_tiles s3://tiles.nst.guide/pct/transit/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalparks.json s3://tiles.nst.guide/pct/transit/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"Transit"},{"location":"cli/quickstart/transit/#transit-layer","text":"Properties: Stops: geometry : location of transit stops (usually Point ) tags : metadata taken straight from GTFS feed _trail : True if the stop was generated from the trail buffer pass _town : True if the stop was generated from the town polygon pass _nearby_stop : True if the stop is within the trail or town buffer, and not another stop on a route Routes: geometry : location of transit routes (usually LineString or MultiLineString ) tags : metadata taken straight from GTFS feed name : name of transit route vehicle_type : type of vehicle used for transit route. Usually bus color : Route color in 6 hexadecimal characters operated_by_name : Name of operator of route _trail : True if the route was generated from the trail buffer pass _town : True if the route was generated from the town polygon pass # Make temp directory mkdir -p tmp # Generate transit python code/main.py export transit \\ ` # trail code, i.e. 'pct'` \\ -t pct \\ ` # file to write transit routes to` \\ --out-routes tmp/transit_routes.geojson \\ ` # file to write transit stops to` \\ --out-stops tmp/transit_stops.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/transit_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/transit_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/transit_routes.geojson\", \"layer\":\"routes\"}' \\ -L '{\"file\":\"tmp/transit_stops.geojson\", \"layer\":\"stops\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Transit' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://transit.land/\" target=\"_blank\">\u00a9 Transitland</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/transit/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/transit.json \\ ` # input JSON file` \\ tmp/transit_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/transit_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/transit/ Add new vector tiles aws s3 cp \\ tmp/nationalparks_tiles s3://tiles.nst.guide/pct/transit/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/nationalparks.json s3://tiles.nst.guide/pct/transit/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"Transit layer"},{"location":"cli/quickstart/wikipedia/","text":"Wikipedia layer \u00b6 The wikipedia layer is constructed by repeatedly calling Wikipedia's Geosearch API . The Geosearch API doesn't allow providing a polygon geometry; you can only provide a point and a radius. To work around this, I construct a minimal set of circles of max radius 10km that fully cover the trail with the provided buffer distance. The result of wikipedia-for-trail is a GeoJSON file where the geometries are Point s, and where the properties have the desired attributes from the -a flag. mkdir -p tmp # Entry point python code/main.py export wikipedia \\ ` # select the PCT; at this point the only valid option ` \\ --trail-code pct \\ ` # provide buffer distance in miles` \\ --buffer 2 \\ ` # Selected attributes` \\ -a best_image -a summary -a title -a url > tmp/wikipedia.geojson Compress this GeoJSON with brotli compression. brotli -c tmp/wikipedia.geojson > tmp/wikipedia_compressed.geojson Then upload this to S3 aws s3 cp \\ tmp/wikipedia_compressed.geojson s3://tiles.nst.guide/pct/wikipedia.geojson \\ --content-type application/geo+json \\ --content-encoding br \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\"","title":"Wikipedia"},{"location":"cli/quickstart/wikipedia/#wikipedia-layer","text":"The wikipedia layer is constructed by repeatedly calling Wikipedia's Geosearch API . The Geosearch API doesn't allow providing a polygon geometry; you can only provide a point and a radius. To work around this, I construct a minimal set of circles of max radius 10km that fully cover the trail with the provided buffer distance. The result of wikipedia-for-trail is a GeoJSON file where the geometries are Point s, and where the properties have the desired attributes from the -a flag. mkdir -p tmp # Entry point python code/main.py export wikipedia \\ ` # select the PCT; at this point the only valid option ` \\ --trail-code pct \\ ` # provide buffer distance in miles` \\ --buffer 2 \\ ` # Selected attributes` \\ -a best_image -a summary -a title -a url > tmp/wikipedia.geojson Compress this GeoJSON with brotli compression. brotli -c tmp/wikipedia.geojson > tmp/wikipedia_compressed.geojson Then upload this to S3 aws s3 cp \\ tmp/wikipedia_compressed.geojson s3://tiles.nst.guide/pct/wikipedia.geojson \\ --content-type application/geo+json \\ --content-encoding br \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\"","title":"Wikipedia layer"},{"location":"cli/quickstart/wilderness/","text":"Wilderness layer \u00b6 Properties: url : url to wilderness.net page for the wilderness name : full name for wilderness acreage : # of acres in wilderness descriptio : Wilderness.net description. Note that this is cut off; after a given number of characters it just has ... . agency : the agency that runs the wilderness. For the PCT dataset it appears only values of NPS , FS , and BLM exist. yeardesign : year the area was federally designated as wilderness geometry : Polygon/MultiPolygon of wilderness area length : Length of trail inside park (in meters) wiki_image : url to Wikipedia image. I try to select the best image but it can be difficult sometimes. wiki_url : url to wikipedia page wiki_summary : summary of wikipedia page. Usually this is the first paragraph. # Make temp directory mkdir -p tmp # Generate polygons python code/main.py export wilderness \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/wilderness.geojson # Generate labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y name \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/wilderness.geojson > tmp/wilderness_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/wilderness_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/wilderness_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/wilderness.geojson\", \"layer\":\"wilderness\"}' \\ -L '{\"file\":\"tmp/wilderness_label.geojson\", \"layer\":\"wilderness_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Designated Wilderness' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.wilderness.net/\" target=\"_blank\">\u00a9 Wilderness.net</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/wilderness/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/wilderness.json \\ ` # input JSON file` \\ tmp/wilderness_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/wilderness_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/wilderness/ Add new vector tiles aws s3 cp \\ tmp/wilderness_tiles s3://tiles.nst.guide/pct/wilderness/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/wilderness.json s3://tiles.nst.guide/pct/wilderness/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"Designated Wilderness"},{"location":"cli/quickstart/wilderness/#wilderness-layer","text":"Properties: url : url to wilderness.net page for the wilderness name : full name for wilderness acreage : # of acres in wilderness descriptio : Wilderness.net description. Note that this is cut off; after a given number of characters it just has ... . agency : the agency that runs the wilderness. For the PCT dataset it appears only values of NPS , FS , and BLM exist. yeardesign : year the area was federally designated as wilderness geometry : Polygon/MultiPolygon of wilderness area length : Length of trail inside park (in meters) wiki_image : url to Wikipedia image. I try to select the best image but it can be difficult sometimes. wiki_url : url to wikipedia page wiki_summary : summary of wikipedia page. Usually this is the first paragraph. # Make temp directory mkdir -p tmp # Generate polygons python code/main.py export wilderness \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/wilderness.geojson # Generate labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y name \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/wilderness.geojson > tmp/wilderness_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/wilderness_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/wilderness_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/wilderness.geojson\", \"layer\":\"wilderness\"}' \\ -L '{\"file\":\"tmp/wilderness_label.geojson\", \"layer\":\"wilderness_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Designated Wilderness' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.wilderness.net/\" target=\"_blank\">\u00a9 Wilderness.net</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/wilderness/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/wilderness.json \\ ` # input JSON file` \\ tmp/wilderness_tiles/metadata.json # Remove the unneeded `metadata.json` rm tmp/wilderness_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/wilderness/ Add new vector tiles aws s3 cp \\ tmp/wilderness_tiles s3://tiles.nst.guide/pct/wilderness/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/wilderness.json s3://tiles.nst.guide/pct/wilderness/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"Wilderness layer"},{"location":"cli/quickstart/wildfire_historical/","text":"National Parks layer \u00b6 Properties: cols = [ 'year', 'name', 'acres', 'inciwebid', 'geometry', 'length', 'wiki_image', 'wiki_url', 'wiki_summary' ] year : Year of wildfire name : Common name of wildfire, e.g. \"Norse Peak\" acres : Last-updated acres of wildfire inciwebid : ID of inciweb page. Can be used to link to inciweb geometry : Polygon/MultiPolygon of wildfire perimeters length : length in meters of PCT in wildfire perimeter. Does not count alternates. Not all fires have Wikipedia pages, but for some objects they have these properties: wiki_image : Best image in wikipedia wiki_url : URL to wikipedia page wiki_summary : Summary from wikipedia page # Make temp directory mkdir -p tmp # Generate polygons python code/main.py export wildfire-historical \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/wildfire_historical.geojson # generate labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y name \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/wildfire_historical.geojson > tmp/wildfire_historical_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/wildfire_historical_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/wildfire_historical_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/wildfire_historical.geojson\", \"layer\":\"wildfire_historical\"}' \\ -L '{\"file\":\"tmp/wildfire_historical_label.geojson\", \"layer\":\"wildfire_historical_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Historical Wildfires' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nifc.gov/\" target=\"_blank\">\u00a9 NIFC</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/wildfire_historical/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/wildfire_historical.json \\ ` # input JSON file` \\ tmp/wildfire_historical_tiles/metadata.json # remove unneeded metadata.json rm tmp/wildfire_historical_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/wildfire_historical/ Add new vector tiles aws s3 cp \\ tmp/wildfire_historical_tiles s3://tiles.nst.guide/pct/wildfire_historical/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/wildfire_historical.json s3://tiles.nst.guide/pct/wildfire_historical/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"Historical Wildfires"},{"location":"cli/quickstart/wildfire_historical/#national-parks-layer","text":"Properties: cols = [ 'year', 'name', 'acres', 'inciwebid', 'geometry', 'length', 'wiki_image', 'wiki_url', 'wiki_summary' ] year : Year of wildfire name : Common name of wildfire, e.g. \"Norse Peak\" acres : Last-updated acres of wildfire inciwebid : ID of inciweb page. Can be used to link to inciweb geometry : Polygon/MultiPolygon of wildfire perimeters length : length in meters of PCT in wildfire perimeter. Does not count alternates. Not all fires have Wikipedia pages, but for some objects they have these properties: wiki_image : Best image in wikipedia wiki_url : URL to wikipedia page wiki_summary : Summary from wikipedia page # Make temp directory mkdir -p tmp # Generate polygons python code/main.py export wildfire-historical \\ ` # trail code, i.e. 'pct'` \\ -t pct > tmp/wildfire_historical.geojson # generate labels python code/main.py geom polylabel \\ ` # include only the name attribute` \\ -y name \\ ` # only keep labels for polygons that are >=30% of MultiPolygon area` \\ --rank-filter 0 .2 \\ tmp/wildfire_historical.geojson > tmp/wildfire_historical_label.geojson Run tippecanoe on the GeoJSON to create vector tiles rm -rf tmp/wildfire_historical_tiles tippecanoe \\ ` # Guess appropriate max zoom` \\ -zg \\ ` # Export tiles to directory` \\ -e tmp/wildfire_historical_tiles \\ ` # Input geojson` \\ -L '{\"file\":\"tmp/wildfire_historical.geojson\", \"layer\":\"wildfire_historical\"}' \\ -L '{\"file\":\"tmp/wildfire_historical_label.geojson\", \"layer\":\"wildfire_historical_label\"}' Convert the exported metadata.json to a JSON file conforming to the Tile JSON spec python code/main.py util metadata-json-to-tile-json \\ ` # Set tileset name` \\ --name 'Historical Wildfires' \\ ` # Set attribution string` \\ --attribution '<a href=\"https://www.nifc.gov/\" target=\"_blank\">\u00a9 NIFC</a>' \\ ` # tile url paths` \\ --url 'https://tiles.nst.guide/pct/wildfire_historical/{z}/{x}/{y}.pbf' \\ ` # Output file path` \\ -o tmp/wildfire_historical.json \\ ` # input JSON file` \\ tmp/wildfire_historical_tiles/metadata.json # remove unneeded metadata.json rm tmp/wildfire_historical_tiles/metadata.json Remove existing vector tiles aws s3 rm \\ --recursive \\ s3://tiles.nst.guide/pct/wildfire_historical/ Add new vector tiles aws s3 cp \\ tmp/wildfire_historical_tiles s3://tiles.nst.guide/pct/wildfire_historical/ \\ --recursive \\ --content-type application/x-protobuf \\ --content-encoding gzip \\ ` # Set to public read access` \\ --acl public-read \\ ` # two hour cache; one day swr` \\ --cache-control \"public, max-age=7200, stale-while-revalidate=86400\" aws s3 cp \\ tmp/wildfire_historical.json s3://tiles.nst.guide/pct/wildfire_historical/tile.json \\ ` # Set to public read access` \\ --acl public-read \\ --content-type application/json","title":"National Parks layer"}]}